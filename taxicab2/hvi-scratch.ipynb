{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch of hierarchical VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Tuple, Union, TypeVar\n",
    "\n",
    "Action = TypeVar(\"Action\")\n",
    "State = TypeVar(\"State\")\n",
    "StateDist = list[Tuple[State, float]]\n",
    "StateRewardDist = list[Tuple[Tuple[State, float], float]]\n",
    "\n",
    "class MDP:\n",
    "    state_list : list[State]\n",
    "    def actions(self, state: State) -> list[Action]: pass\n",
    "    def next_state_reward_dist(self, state: State, action: Action) -> StateRewardDist: pass\n",
    "    def is_terminal(self, state: State) -> bool: pass\n",
    "\n",
    "class SubTask:\n",
    "    mdp: MDP\n",
    "    def child_subtasks(self, state: State) -> list[Union[\"SubTask\", Action]]: pass\n",
    "    def continuation_prob(self, state: State) -> float: pass\n",
    "    def exit_reward(self, state: State) -> float: pass\n",
    "    def exit_distribution(self, state: State) -> StateDist: pass #eq 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value(subtask: SubTask, s: State):\n",
    "    if subtask.mdp.is_terminal(s):\n",
    "        return 0 # HACK: do we need to add pseudoreward???\n",
    "    continue_prob = subtask.continuation_prob(s)\n",
    "\n",
    "    # max_a \\sum_{s', r} p(s', r | s, a) [r + gamma_j(s)*V(s')]\n",
    "    max_qval = float(\"-inf\")\n",
    "    for a in subtask.child_subtasks(s):\n",
    "        # get next-state/reward distribution for semi-MDP\n",
    "        if isinstance(a, Action):\n",
    "            ns_r_prob = subtask.mdp.next_state_reward_dist(s, a)\n",
    "        elif isinstance(a, SubTask):\n",
    "            ns_r_prob = []\n",
    "            for ns, prob in a.exit_distribution(s):\n",
    "                r = value(a, s) - a.exit_reward(ns)\n",
    "                ns_r_prob.append((ns, r, prob))\n",
    "        # calculate expected value of action Q(subtask, s, a)\n",
    "        qval = 0\n",
    "        for ns, r, prob in ns_r_prob:\n",
    "            qval += prob * (r + continue_prob*value(subtask, ns))\n",
    "        # update max value\n",
    "        max_qval = max(max_qval, qval)\n",
    "\n",
    "    # + (1 - gamma_j(s))*eps_j(s)\n",
    "    max_qval += (1 - continue_prob)*subtask.exit_reward(s)\n",
    "    return max_qval\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
