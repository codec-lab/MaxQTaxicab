{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from ihrl.taxicab import TaxiMDP, Root, taxi_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_str = \"\"\"\n",
    "A--B\n",
    "----\n",
    "----\n",
    "C--D \n",
    "\"\"\"\n",
    "mdp = TaxiMDP(layout_str)\n",
    "state_length = len(mdp.list_all_possible_states())\n",
    "test_root = Root(mdp)\n",
    "init_state = taxi_state(0,0,mdp.width-1,0)\n",
    "init_state_index = mdp.list_all_possible_states().index(init_state)\n",
    "action_length = mdp.action_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#an object just for holding the true transition and reward functions\n",
    "#used for value iteration since the cache cant take in matrices\n",
    "tr = mdp.get_transition_reward_obj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the state and action indices of the total trajectories \n",
    "state_indices, action_indices = mdp.get_state_action_indices(total_trajectories=1000,max_t_length=110,transition_reward_obj=tr,deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix_true = torch.tensor(mdp.get_transition_matrix())\n",
    "reward_weights_true = torch.tensor(mdp.get_reward_matrix(),dtype=torch.float32)\n",
    "state_indices = torch.tensor(state_indices)\n",
    "action_indices = torch.tensor(action_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_vi(\n",
    "    transition_matrix : torch.Tensor, #Input to be learened\n",
    "    reward_weights : torch.Tensor,\n",
    "    discount_rate : float,\n",
    "    entropy_bonus : float,\n",
    "    precision : float = 1e-2,\n",
    ") -> torch.Tensor:\n",
    "    assert 0 <= discount_rate < 1\n",
    "    state_values = torch.zeros_like(reward_weights[:,0])\n",
    "    action_value = torch.zeros_like(transition_matrix[:, :, 0])\n",
    "    for i in range(1): \n",
    "        next_state_value = torch.einsum(\"san,n->sa\", transition_matrix, state_values) #T(s'| s,a)vb(s')\n",
    "        #compute val of each [(p(s'| a1,s) * vb(s')) , ... , (p(s'| a6,s) * vb(s'))]\n",
    "\n",
    "        action_value = reward_weights + discount_rate * next_state_value #R(s,a) + gamma * T(s'| s,a)vb(s')\n",
    "        #[R(s,a1) + V(s'), ... , R(s,a6) * V(s')]\n",
    "        new_state_values = entropy_bonus * torch.logsumexp((1/entropy_bonus)*action_value, dim=1) #log(sum(a)exp(1/b * Q(s,a)))\n",
    "\n",
    "        state_values = new_state_values\n",
    "        if torch.max(abs(new_state_values - state_values)) <  precision:\n",
    "            break\n",
    "    policy = torch.softmax(action_value*(1/entropy_bonus), dim=1)\n",
    "    return policy, state_values\n",
    "\n",
    "def maximum_likelihood_irl(\n",
    "    mdp : TaxiMDP,\n",
    "    discount_rate : float,\n",
    "    state_indices : torch.Tensor, \n",
    "    action_indices :   torch.Tensor,\n",
    "    iterations: int,\n",
    "    entropy_bonus: float\n",
    "):\n",
    "    state_length = len(mdp.list_all_possible_states())\n",
    "    action_length = len(mdp.actions(init_state))\n",
    "\n",
    "    transition_weights = torch.rand(state_length, action_length, state_length)\n",
    "    transition_weights.requires_grad_(True)\n",
    "\n",
    "    #rewards should just be a reward for each state\n",
    "    reward_tensor = -torch.rand(state_length,action_length)\n",
    "    reward_tensor.requires_grad_(True)\n",
    "\n",
    "    discount_rate = discount_rate\n",
    "    optimizer = torch.optim.Adam([reward_tensor,transition_weights], lr=1e-3)#, weight_decay=1e-4)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        #reward_weights = reward_weights.clamp(min = -10, max = 15) #same min,max as actual r to make comparing easier\n",
    "        #transition is always bw 0 and 1\n",
    "        #transition_matrix = transition_matrix_true# uncomment and change other parts to use true transition matrix\n",
    "        transition_matrix = torch.softmax(transition_weights,dim=-1)\n",
    "        reward_weights = reward_tensor\n",
    "        policy_matrix, _ = soft_vi(\n",
    "            transition_matrix,\n",
    "            reward_weights=reward_weights,\n",
    "            discount_rate=discount_rate,\n",
    "            entropy_bonus=entropy_bonus\n",
    "        )\n",
    "\n",
    "        policy_loss = -torch.log(policy_matrix[state_indices, action_indices]).sum() / (len(state_indices))\n",
    "        #print('policy loss', policy_loss)\n",
    "        transition_loss = -torch.log((transition_matrix[state_indices[:-1],action_indices[:-1],state_indices[1:]])+1e-6).sum() / len(state_indices)\n",
    "        #print('t loss',transition_loss)\n",
    "        loss = policy_loss + transition_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if torch.isnan(loss):\n",
    "            break\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Loss: {loss.item()} at iteration {i}\")\n",
    "    print(f\"Final Loss: {loss.item()}\")\n",
    "    return reward_weights.detach(), transition_matrix.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.621004104614258 at iteration 0\n",
      "Loss: 4.308089256286621 at iteration 500\n",
      "Loss: 3.34889817237854 at iteration 1000\n",
      "Loss: 2.5795114040374756 at iteration 1500\n",
      "Loss: 1.9620102643966675 at iteration 2000\n",
      "Loss: 1.525149941444397 at iteration 2500\n",
      "Loss: 1.2516618967056274 at iteration 3000\n",
      "Loss: 1.090242862701416 at iteration 3500\n",
      "Loss: 0.994340181350708 at iteration 4000\n",
      "Loss: 0.9351127743721008 at iteration 4500\n",
      "Loss: 0.896856427192688 at iteration 5000\n",
      "Loss: 0.8711298704147339 at iteration 5500\n",
      "Loss: 0.8532452583312988 at iteration 6000\n",
      "Loss: 0.8404781818389893 at iteration 6500\n",
      "Final Loss: 0.8311876058578491\n"
     ]
    }
   ],
   "source": [
    "reward_weights_est,transition_matrix_est = maximum_likelihood_irl(mdp,0.9,state_indices,action_indices,7000,0.1)\n",
    "policy_matrix_est, state_values_est = soft_vi(transition_matrix_est,reward_weights_est,0.9,0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy matrix appears to overfit. It shouldn't be able to perfectly predict actions, or be over 90% accurate because of the randomness in trajectories when multiple best actions are available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9143)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_matrix_est[state_indices,action_indices].sum() / len(state_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mismatch\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=3, y=1), passenger=Passenger(location=None, destination=Location(x=0, y=0))), waiting_passengers=())\n",
      "True: 3 Est: 1\n",
      "mismatch\n",
      "[0.   0.31 0.   0.69 0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=3, y=3), passenger=Passenger(location=None, destination=Location(x=0, y=0))), waiting_passengers=())\n",
      "True: 1 Est: 3\n",
      "mismatch\n",
      "[1. 0. 0. 0. 0. 0.]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=1, y=3), passenger=None), waiting_passengers=(Passenger(location=Location(x=3, y=0), destination=None),))\n",
      "True: 3 Est: 0\n",
      "mismatch\n",
      "[0.   0.58 0.42 0.   0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=2, y=2), passenger=None), waiting_passengers=(Passenger(location=Location(x=0, y=3), destination=None),))\n",
      "True: 2 Est: 1\n",
      "mismatch\n",
      "[0.   0.31 0.69 0.   0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=2, y=1), passenger=Passenger(location=None, destination=Location(x=0, y=3))), waiting_passengers=())\n",
      "True: 1 Est: 2\n",
      "mismatch\n",
      "[0.61 0.   0.39 0.   0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=2, y=1), passenger=Passenger(location=None, destination=Location(x=3, y=3))), waiting_passengers=())\n",
      "True: 2 Est: 0\n",
      "mismatch\n",
      "[0.   0.43 0.57 0.   0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=3, y=0), passenger=Passenger(location=None, destination=Location(x=0, y=3))), waiting_passengers=())\n",
      "True: 1 Est: 2\n",
      "mismatch\n",
      "[0.   0.13 0.87 0.   0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=1, y=1), passenger=Passenger(location=None, destination=Location(x=0, y=3))), waiting_passengers=())\n",
      "True: 1 Est: 2\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=0, y=0), passenger=None), waiting_passengers=())\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=3, y=0), passenger=None), waiting_passengers=())\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=0, y=3), passenger=None), waiting_passengers=())\n",
      "mismatch\n",
      "[0.14 0.   0.   0.86 0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=1, y=1), passenger=Passenger(location=None, destination=Location(x=3, y=0))), waiting_passengers=())\n",
      "True: 0 Est: 3\n",
      "mismatch\n",
      "[0.79 0.   0.   0.21 0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=0, y=2), passenger=Passenger(location=None, destination=Location(x=3, y=0))), waiting_passengers=())\n",
      "True: 3 Est: 0\n",
      "mismatch\n",
      "[0.32 0.   0.   0.68 0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=0, y=1), passenger=None), waiting_passengers=(Passenger(location=Location(x=3, y=0), destination=None),))\n",
      "True: 0 Est: 3\n",
      "mismatch\n",
      "[1. 0. 0. 0. 0. 0.]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=2, y=2), passenger=Passenger(location=None, destination=Location(x=3, y=3))), waiting_passengers=())\n",
      "True: 2 Est: 0\n",
      "mismatch\n",
      "[0. 0. 0. 1. 0. 0.]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=0, y=3), passenger=None), waiting_passengers=(Passenger(location=Location(x=3, y=0), destination=None),))\n",
      "True: 0 Est: 3\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=3, y=3), passenger=None), waiting_passengers=())\n",
      "mismatch\n",
      "[0. 0. 0. 1. 0. 0.]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=1, y=2), passenger=Passenger(location=None, destination=Location(x=0, y=0))), waiting_passengers=())\n",
      "True: 1 Est: 3\n",
      "mismatch\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=2, y=2), passenger=None), waiting_passengers=(Passenger(location=Location(x=0, y=0), destination=None),))\n",
      "True: 3 Est: 1\n",
      "mismatch\n",
      "[0.67 0.   0.33 0.   0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=2, y=2), passenger=None), waiting_passengers=(Passenger(location=Location(x=3, y=3), destination=None),))\n",
      "True: 2 Est: 0\n",
      "mismatch\n",
      "[0.56 0.   0.44 0.   0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=0, y=0), passenger=Passenger(location=None, destination=Location(x=3, y=3))), waiting_passengers=())\n",
      "True: 2 Est: 0\n",
      "mismatch\n",
      "[0.   0.55 0.45 0.   0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=3, y=1), passenger=Passenger(location=None, destination=Location(x=0, y=3))), waiting_passengers=())\n",
      "True: 2 Est: 1\n",
      "mismatch\n",
      "[0. 0. 1. 0. 0. 0.]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=2, y=0), passenger=Passenger(location=None, destination=Location(x=3, y=3))), waiting_passengers=())\n",
      "True: 0 Est: 2\n",
      "mismatch\n",
      "[0.64 0.   0.36 0.   0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=0, y=1), passenger=Passenger(location=None, destination=Location(x=3, y=3))), waiting_passengers=())\n",
      "True: 2 Est: 0\n",
      "mismatch\n",
      "[1. 0. 0. 0. 0. 0.]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=2, y=2), passenger=None), waiting_passengers=(Passenger(location=Location(x=3, y=0), destination=None),))\n",
      "True: 3 Est: 0\n",
      "mismatch\n",
      "[0.   0.54 0.   0.46 0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=1, y=2), passenger=None), waiting_passengers=(Passenger(location=Location(x=0, y=0), destination=None),))\n",
      "True: 3 Est: 1\n",
      "mismatch\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=1, y=3), passenger=Passenger(location=None, destination=Location(x=0, y=0))), waiting_passengers=())\n",
      "True: 3 Est: 1\n",
      "mismatch\n",
      "[0. 0. 0. 1. 0. 0.]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=2, y=1), passenger=None), waiting_passengers=(Passenger(location=Location(x=0, y=0), destination=None),))\n",
      "True: 1 Est: 3\n",
      "mismatch\n",
      "[0. 0. 1. 0. 0. 0.]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=2, y=2), passenger=Passenger(location=None, destination=Location(x=0, y=3))), waiting_passengers=())\n",
      "True: 1 Est: 2\n",
      "mismatch\n",
      "[0.   0.61 0.39 0.   0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=2, y=1), passenger=None), waiting_passengers=(Passenger(location=Location(x=0, y=3), destination=None),))\n",
      "True: 2 Est: 1\n",
      "mismatch\n",
      "[0.   0.75 0.   0.25 0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=3, y=2), passenger=Passenger(location=None, destination=Location(x=0, y=0))), waiting_passengers=())\n",
      "True: 3 Est: 1\n",
      "mismatch\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=3, y=2), passenger=None), waiting_passengers=(Passenger(location=Location(x=0, y=3), destination=None),))\n",
      "True: 2 Est: 1\n",
      "mismatch\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=1, y=3), passenger=None), waiting_passengers=(Passenger(location=Location(x=0, y=0), destination=None),))\n",
      "True: 3 Est: 1\n",
      "mismatch\n",
      "[0.74 0.   0.   0.26 0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=2, y=2), passenger=Passenger(location=None, destination=Location(x=3, y=0))), waiting_passengers=())\n",
      "True: 3 Est: 0\n",
      "mismatch\n",
      "[0.   0.61 0.39 0.   0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=2, y=0), passenger=Passenger(location=None, destination=Location(x=0, y=3))), waiting_passengers=())\n",
      "True: 2 Est: 1\n",
      "mismatch\n",
      "[0.39 0.   0.   0.61 0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=1, y=2), passenger=Passenger(location=None, destination=Location(x=3, y=0))), waiting_passengers=())\n",
      "True: 0 Est: 3\n",
      "mismatch\n",
      "[0.42 0.   0.58 0.   0.   0.  ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=1, y=2), passenger=Passenger(location=None, destination=Location(x=3, y=3))), waiting_passengers=())\n",
      "True: 0 Est: 2\n",
      "mismatch\n",
      "[0.  0.5 0.5 0.  0.  0. ]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=1, y=0), passenger=Passenger(location=None, destination=Location(x=0, y=3))), waiting_passengers=())\n",
      "True: 2 Est: 1\n",
      "mismatch\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=1, y=1), passenger=Passenger(location=None, destination=Location(x=0, y=0))), waiting_passengers=())\n",
      "True: 3 Est: 1\n",
      "mismatch\n",
      "[0. 0. 1. 0. 0. 0.]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=3, y=0), passenger=None), waiting_passengers=(Passenger(location=Location(x=0, y=3), destination=None),))\n",
      "True: 1 Est: 2\n",
      "mismatch\n",
      "[1. 0. 0. 0. 0. 0.]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=0, y=2), passenger=Passenger(location=None, destination=Location(x=3, y=3))), waiting_passengers=())\n",
      "True: 2 Est: 0\n",
      "mismatch\n",
      "[0. 0. 0. 1. 0. 0.]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=2, y=1), passenger=Passenger(location=None, destination=Location(x=3, y=0))), waiting_passengers=())\n",
      "True: 0 Est: 3\n",
      "mismatch\n",
      "[0. 0. 0. 1. 0. 0.]\n",
      "TaxiCabState(taxi=Taxi(location=Location(x=3, y=3), passenger=None), waiting_passengers=(Passenger(location=Location(x=0, y=0), destination=None),))\n",
      "True: 1 Est: 3\n",
      "Accuracy: 0.6742424242424242\n"
     ]
    }
   ],
   "source": [
    "matches = 0\n",
    "for i in range(len(mdp.list_all_possible_states())):\n",
    "    true_max_action = mdp.value_iteration(i,110,tr)[1]\n",
    "    if true_max_action == None:\n",
    "        print(mdp.list_all_possible_states()[i]) #these are the terminal states that will never have an action follow them\n",
    "        continue\n",
    "    true_max_action_index = mdp.actions().index(true_max_action)\n",
    "    if torch.argmax(policy_matrix_est[i]) == true_max_action_index:\n",
    "        matches += 1\n",
    "    else:\n",
    "        print('mismatch')\n",
    "        print(np.round(policy_matrix_est[i].detach().numpy(),2))\n",
    "        print(mdp.list_all_possible_states()[i])\n",
    "        print(f\"True: {true_max_action_index} Est: {torch.argmax(policy_matrix_est[i])}\")\n",
    "print(f\"Accuracy: {matches/len(mdp.list_all_possible_states())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_val_est = (reward_weights_est + torch.einsum(\"san,n->sa\", transition_matrix_true, state_values_est))\n",
    "sv_est = torch.logsumexp((1/0.1)*action_val_est,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_state_vals = []\n",
    "for i in range(len(mdp.list_all_possible_states())):\n",
    "    true_state_vals.append(mdp.value_iteration(i,100,tr)[0])\n",
    "true_state_vals = torch.tensor(true_state_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there doesn't seem to be a correspondence between the state values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110    -2.741294\n",
       "102    -2.741294\n",
       "44     -2.741294\n",
       "128    -2.741294\n",
       "81     -1.934771\n",
       "         ...    \n",
       "14     11.272727\n",
       "26     13.636364\n",
       "126    13.636364\n",
       "60     13.636364\n",
       "68     13.636364\n",
       "Length: 132, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(true_state_vals).sort_values() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     -3.555149\n",
       "76    -2.688258\n",
       "10    -2.605184\n",
       "118   -2.545925\n",
       "21    -2.523063\n",
       "         ...   \n",
       "77     6.395243\n",
       "54     6.564583\n",
       "128    7.248069\n",
       "117    7.346713\n",
       "113    7.571374\n",
       "Length: 132, dtype: float32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(sv_est).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "<ol>\n",
    "<li>Reward argmax matches</li>\n",
    "<li>[Transition matrix * Reward Weight] argmax matches</li>\n",
    "<li>Transition matrix argmax matches</li>\n",
    "</ol>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 39/132\n"
     ]
    }
   ],
   "source": [
    "matches = 0\n",
    "for i in range(len(reward_weights_est)):\n",
    "    if reward_weights_est[i].argmax() == reward_weights_true[i].argmax():\n",
    "        matches += 1\n",
    "print(f\"Matches: {matches}/{len(reward_weights_est)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 3/792\n"
     ]
    }
   ],
   "source": [
    "#transition * reward weights est \n",
    "#state 0, action 0\n",
    "matches = 0\n",
    "for i in range(state_length): \n",
    "    for a in range(action_length):\n",
    "        if (transition_matrix_est[i][a] * reward_weights_est[i][a]).argmax() == (transition_matrix_true[i][a] * reward_weights_true[i][a]).argmax():\n",
    "            matches += 1\n",
    "print(f\"Matches: {matches}/{state_length*action_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 161/792\n"
     ]
    }
   ],
   "source": [
    "#transition_matrix\n",
    "matches = 0\n",
    "for i in range(state_length): \n",
    "    for a in range(action_length):\n",
    "        if (transition_matrix_est[i][a]).argmax() == (transition_matrix_true[i][a]).argmax():\n",
    "            matches += 1\n",
    "print(f\"Matches: {matches}/{state_length*action_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ihrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
